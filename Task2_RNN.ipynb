{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1ZwAooHmpuZ",
        "outputId": "d7c37daf-3998-44e9-9257-0fb536daba45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train = torch.load(\"/content/drive/MyDrive/Colab Notebooks/DL1/train.pt\")\n",
        "valid = torch.load(\"/content/drive/MyDrive/Colab Notebooks/DL1/val.pt\")\n",
        "test = torch.load(\"/content/drive/MyDrive/Colab Notebooks/DL1/test.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U6FztxunOWz",
        "outputId": "3bb8048d-e42d-4ce5-eb82-653af73b2495"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-362843f868f5>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train = torch.load(\"/content/drive/MyDrive/Colab Notebooks/DL1/train.pt\")\n",
            "<ipython-input-2-362843f868f5>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  valid = torch.load(\"/content/drive/MyDrive/Colab Notebooks/DL1/val.pt\")\n",
            "<ipython-input-2-362843f868f5>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test = torch.load(\"/content/drive/MyDrive/Colab Notebooks/DL1/test.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Knowing the shapes of dataset\n",
        "print(\"Shape of tensor_data train:\", train['samples'].shape)\n",
        "print(\"Shape of tensor_data valid:\", valid['samples'].shape)\n",
        "print(\"Shape of tensor_data test:\", test['samples'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIZFtTErpGH_",
        "outputId": "815f4ca2-5214-4e68-ebc8-db61a8995f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor_data train: torch.Size([5881, 3, 206])\n",
            "Shape of tensor_data valid: torch.Size([1471, 3, 206])\n",
            "Shape of tensor_data test: torch.Size([2947, 3, 206])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''1. Dimension 1: 5881 (train), 1471 (valid), 2947 (test)\n",
        "\n",
        "This dimension represents the number of samples or data points in each of your datasets\n",
        "(training, validation, and testing).\n",
        "\n",
        "There are 5881 samples in the training dataset.\n",
        "There are 1471 samples in the validation dataset.\n",
        "There are 2947 samples in the testing dataset.\n",
        "\n",
        "2. Dimension 2: 3\n",
        "this dimension most likely represents the 3-axis accelerometer data.\n",
        "Accelerometers typically measure acceleration in three directions:\n",
        "\n",
        "X-axis: Movement along the horizontal axis.\n",
        "Y-axis: Movement along the vertical axis.\n",
        "Z-axis: Movement along the depth axis.\n",
        "\n",
        "3. Dimension 3: 206\n",
        "\n",
        "This dimension represents the number of time steps or readings within a single sample or window of activity.\n",
        "This could be interpreted as a time window of 206 sensor samples'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "NjXlOU7wsMfM",
        "outputId": "41253c95-7ed4-4137-9633-dc800e442e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. Dimension 1: 5881 (train), 1471 (valid), 2947 (test)\\n\\nThis dimension represents the number of samples or data points in each of your datasets \\n(training, validation, and testing).\\n\\nThere are 5881 samples in the training dataset.\\nThere are 1471 samples in the validation dataset.\\nThere are 2947 samples in the testing dataset.\\n\\n2. Dimension 2: 3\\nthis dimension most likely represents the 3-axis accelerometer data.\\nAccelerometers typically measure acceleration in three directions:\\n\\nX-axis: Movement along the horizontal axis.\\nY-axis: Movement along the vertical axis.\\nZ-axis: Movement along the depth axis.\\n\\n3. Dimension 3: 206\\n\\nThis dimension represents the number of time steps or readings within a single sample or window of activity.\\nThis could be interpreted as a time window of 206 sensor samples'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract samples and labels\n",
        "train_samples, train_labels = train['samples'], train['labels']\n",
        "valid_samples, valid_labels = valid['samples'], valid['labels']\n",
        "test_samples, test_labels = test['samples'], test['labels']\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(train_samples, train_labels)\n",
        "valid_dataset = TensorDataset(valid_samples, valid_labels)\n",
        "test_dataset = TensorDataset(test_samples, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Xm42MLyws_7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model\n",
        "class HAR_RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(HAR_RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "uygosg211-g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, optimizer, and loss function\n",
        "input_size = 206  # 3 accelerometer channels\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 6  # 6 activity classes\n",
        "\n",
        "model = HAR_RNN(input_size, hidden_size, num_layers, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 35\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        target = target.type(torch.LongTensor)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_loader:\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    valid_acc = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {valid_acc:.2f}%\")\n",
        "\n",
        "    # Save best model checkpoint\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Load best model and evaluate on test set\n",
        "model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu'),weights_only=True), strict=False)\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)  # Call torch.max with output and unpack\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "test_acc = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7fixqYW2Cp3",
        "outputId": "12be25d5-31e8-4541-abaa-05495bb4478e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/35], Validation Accuracy: 53.30%\n",
            "Epoch [2/35], Validation Accuracy: 61.05%\n",
            "Epoch [3/35], Validation Accuracy: 59.01%\n",
            "Epoch [4/35], Validation Accuracy: 58.33%\n",
            "Epoch [5/35], Validation Accuracy: 61.05%\n",
            "Epoch [6/35], Validation Accuracy: 61.59%\n",
            "Epoch [7/35], Validation Accuracy: 63.29%\n",
            "Epoch [8/35], Validation Accuracy: 77.02%\n",
            "Epoch [9/35], Validation Accuracy: 69.41%\n",
            "Epoch [10/35], Validation Accuracy: 85.32%\n",
            "Epoch [11/35], Validation Accuracy: 87.97%\n",
            "Epoch [12/35], Validation Accuracy: 87.56%\n",
            "Epoch [13/35], Validation Accuracy: 88.38%\n",
            "Epoch [14/35], Validation Accuracy: 89.67%\n",
            "Epoch [15/35], Validation Accuracy: 90.01%\n",
            "Epoch [16/35], Validation Accuracy: 89.94%\n",
            "Epoch [17/35], Validation Accuracy: 89.12%\n",
            "Epoch [18/35], Validation Accuracy: 91.64%\n",
            "Epoch [19/35], Validation Accuracy: 90.96%\n",
            "Epoch [20/35], Validation Accuracy: 91.98%\n",
            "Epoch [21/35], Validation Accuracy: 91.91%\n",
            "Epoch [22/35], Validation Accuracy: 92.25%\n",
            "Epoch [23/35], Validation Accuracy: 92.39%\n",
            "Epoch [24/35], Validation Accuracy: 92.52%\n",
            "Epoch [25/35], Validation Accuracy: 92.66%\n",
            "Epoch [26/35], Validation Accuracy: 91.84%\n",
            "Epoch [27/35], Validation Accuracy: 92.45%\n",
            "Epoch [28/35], Validation Accuracy: 93.41%\n",
            "Epoch [29/35], Validation Accuracy: 93.81%\n",
            "Epoch [30/35], Validation Accuracy: 94.49%\n",
            "Epoch [31/35], Validation Accuracy: 93.27%\n",
            "Epoch [32/35], Validation Accuracy: 94.02%\n",
            "Epoch [33/35], Validation Accuracy: 94.77%\n",
            "Epoch [34/35], Validation Accuracy: 93.95%\n",
            "Epoch [35/35], Validation Accuracy: 93.47%\n",
            "Test Accuracy: 81.30%\n"
          ]
        }
      ]
    }
  ]
}